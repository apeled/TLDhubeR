{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This file creates and stores an VectorStoreIndex for usage in our streamlit application.\n",
    "\n",
    "Notebook contents:\n",
    "1. Load JSON transcripts.\n",
    "2. Parse transcript sections into Documents and attaches metadata.\n",
    "3. Extract keywords and embeds nodes using the OpenAI API.\n",
    "4. Create VectorStoreIndex from the nodes and stores it locally.\n",
    "5. Test reloading the index.\n",
    "6. Create and test a simple retrieval engine using embeddings.\n",
    "\n",
    "Modules: os, json, time, nest_asyncio, pickle, llama_index libraries.\n",
    "\n",
    "Author: Edouard Seryozhenkov\n",
    "Date: 2024-02-29\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import nest_asyncio\n",
    "import pickle as pkl\n",
    "\n",
    "from llama_index.core import Document, VectorStoreIndex, get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core.extractors import (\n",
    "    KeywordExtractor,\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.schema import MetadataMode\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_transcripts(base_path: str) -> list:\n",
    "    \"\"\"Loads all JSON transcript files from the specified base path.\n",
    "\n",
    "    Args:\n",
    "        base_path (str): The path to the directory containing the transcript files.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing all parsed transcript data as JSON objects.\n",
    "            An empty list is returned if no valid transcripts are found.\n",
    "    \"\"\"\n",
    "    podcasts_as_json = []\n",
    "    for filename in os.listdir(base_path):\n",
    "        if filename.endswith(\".json\"):\n",
    "            path = base_path + \"/\" + filename\n",
    "            with open(path, \"r\") as file:\n",
    "                podcasts_as_json.append(json.load(file))\n",
    "\n",
    "    return podcasts_as_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_into_documents(podcast_jsons: list) -> list[Document]:\n",
    "    \"\"\"Parses a list of JSON transcripts into a list of Document objects.\n",
    "\n",
    "    For each transcript:\n",
    "\n",
    "    1. Extracts essential metadata like episode title, number, summary, and YouTube link.\n",
    "    2. Creates a list of document subsections (\"chunks\") from the transcript's \"chunks\" key.\n",
    "    3. For each chunk:\n",
    "        - Creates a copy of the transcript metadata to avoid modifying the original.\n",
    "        - Adds the chunk's timestamp to the metadata.\n",
    "        - Initializes a Document object with the chunk's text and the modified metadata.\n",
    "        - Excludes specified metadata keys (episode_summary, timestamp, youtube_link) from\n",
    "            both LLM and embed processing within the Document object.\n",
    "        - Appends the created Document object to a list.\n",
    "\n",
    "    Finally, the function returns the constructed list of Document objects.\n",
    "\n",
    "    Args:\n",
    "        podcast_jsons (list): A list of JSON objects, each representing a podcast transcript.\n",
    "\n",
    "    Returns:\n",
    "        list[Document]: A list of Document objects containing parsed transcript data and \n",
    "                            metadata from each input JSON. An empty list is returned if no valid \n",
    "                            transcripts are provided.\n",
    "    \"\"\"\n",
    "    doc_list = []\n",
    "    for pc_json in podcast_jsons:\n",
    "        podcast_metadata = {\n",
    "            \"episode_title\": pc_json[\"title\"],\n",
    "            \"episode_number\": pc_json[\"ep_num\"],\n",
    "            \"episode_summary\": pc_json[\"episode_summary\"],\n",
    "            \"youtube_link\": pc_json[\"link\"],\n",
    "        }\n",
    "        chunk_list = [chunk for chunk in pc_json[\"chunks\"]]\n",
    "        for chunk in chunk_list:\n",
    "            chunk_metadata = podcast_metadata.copy()\n",
    "            chunk_metadata[\"timestamp\"] = chunk[\"timestamp\"]\n",
    "            chunk_text = chunk[\"text\"]\n",
    "            doc = Document(text=chunk_text, metadata=chunk_metadata)\n",
    "            doc.excluded_embed_metadata_keys = [\n",
    "                \"episode_summary\",\n",
    "                \"timestamp\",\n",
    "                \"youtube_link\",\n",
    "            ]\n",
    "            doc.excluded_llm_metadata_keys = [\n",
    "                \"episode_summary\",\n",
    "                \"timestamp\",\n",
    "                \"youtube_link\",\n",
    "            ]\n",
    "            doc_list.append(doc)\n",
    "    return doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_hube_engine(documents):\n",
    "    \"\"\"Makes a simple query engine that uses only the embeddings. Returns\n",
    "        the context that would be fetched for the LLM, up to 20 nodes.\n",
    "\n",
    "    Args:\n",
    "        documents: A list of Document objects corresponding to podcast subsections\n",
    "\n",
    "    Returns:\n",
    "        query engine: An engine in \"no_text\" response mode\n",
    "    \"\"\"\n",
    "    # Build a simple index from documents\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "    # Configure a retriever using this index\n",
    "    retriever = VectorIndexRetriever(\n",
    "        index=index,\n",
    "        similarity_top_k=20,\n",
    "    )\n",
    "\n",
    "    # Configure response synthesizer\n",
    "    response_synthesizer = get_response_synthesizer(response_mode=\"no_text\")\n",
    "\n",
    "    # Assemble the query engine\n",
    "    simple_hube_engine = RetrieverQueryEngine.from_args(\n",
    "        retriever=retriever,\n",
    "        response_synthesizer=response_synthesizer,\n",
    "        node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    "    )\n",
    "\n",
    "    return simple_hube_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This cell contains helper functions to serialize and load nodes from storage\"\"\"\n",
    "def dump_object(x, filename=\"x.pkl\"):\n",
    "    # Writes Python object to a file using pickle\n",
    "    with open(filename, \"wb\") as file:\n",
    "        pkl.dump(x, file)\n",
    "\n",
    "def load_object(filename):\n",
    "    with open(filename, \"rb\") as file:\n",
    "        obj = pkl.load(file)\n",
    "        return obj\n",
    "\n",
    "def unpickle_nodes(base_path):\n",
    "    unpickled_nodes = []\n",
    "    for filename in os.listdir(base_path):\n",
    "        if filename.endswith(\".pkl\"):\n",
    "            path = os.path.join(base_path, filename)\n",
    "            nodes = load_object(path)\n",
    "            unpickled_nodes.extend(nodes)\n",
    "    return unpickled_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents(\n",
    "    documents: list[Document],\n",
    "    start_index: int = 0,\n",
    "    batch_size: int = 15,\n",
    ") -> int:\n",
    "    \"\"\"Processes a list of Document objects using a ingestion pipeline, including:\n",
    "\n",
    "    1. Sentence splitting: Splits documents into smaller sentences for easier processing.\n",
    "    2. Keyword extraction: Extracts the top keywords from each document.\n",
    "    3. OpenAI embedding: Generates embeddings for each document using the specified OpenAI model.\n",
    "\n",
    "    Each batch is processed using the `IngestionPipeline`, and the resulting data is then serialized.\n",
    "\n",
    "    Args:\n",
    "        documents (list[Document]): A list of documents to be processed.\n",
    "        start_index (int, optional): The index at which to start processing. Defaults to 0.\n",
    "        batch_size (int, optional): The number of documents to process in each batch.\n",
    "                                    Defaults to 15.\n",
    "\n",
    "    Returns:\n",
    "        int: Returns 0 to indicate successful completion.\n",
    "    \"\"\"\n",
    "    my_transformations = [\n",
    "        SentenceSplitter(chunk_size=1024),\n",
    "        KeywordExtractor(keywords=5),\n",
    "        OpenAIEmbedding(model=\"text-embedding-3-small\"),\n",
    "    ]\n",
    "    pipeline = IngestionPipeline(transformations=my_transformations)\n",
    "    for i in range(start_index, len(documents), batch_size):\n",
    "        if i + batch_size < len(documents):\n",
    "            batch = documents[i : i + batch_size]\n",
    "            nodes = pipeline.run(documents=batch)\n",
    "            dump_object(nodes, filename=f\"nodes_{i}.pkl\")\n",
    "        else:\n",
    "            # Last batch\n",
    "            batch = documents[i:]\n",
    "            nodes = pipeline.run(documents=batch)\n",
    "            dump_object(nodes, filename=f\"nodes_final.pkl\")\n",
    "        # Wait to avoid exceeding OpenAI rate limits\n",
    "        time.sleep(60)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mid_video_link(link, t):\n",
    "    \"\"\"Modifies YouTube link to start at the specified time (seconds).\"\"\"\n",
    "    base_url = link.replace(\"www.youtube.com/watch?v=\", \"youtu.be/\")\n",
    "    return base_url + \"?t=\" + str(t)\n",
    "\n",
    "\n",
    "def extract_metadata(response):\n",
    "    \"\"\"Extracts and transforms metadata from source nodes in a query response. \"\"\"\n",
    "    metadata_list = [node.metadata for node in response.source_nodes]\n",
    "    for metadata in metadata_list:\n",
    "        base_link = metadata[\"youtube_link\"]\n",
    "        start_time = metadata[\"timestamp\"]\n",
    "        metadata[\"youtube_link\"] = get_mid_video_link(base_link, start_time)\n",
    "    return metadata_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using GPT-3.5 for keyword extraction because it is cheaper\n",
    "Settings.llm = OpenAI(temperature=0.2, model=\"gpt-3.5-turbo-0125\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the transcripts into Documents\n",
    "podcast_jsons = load_json_transcripts(\"/home/edouas/DATA-515/TLDhubeR/transcript_data\")\n",
    "documents = parse_into_documents(podcast_jsons)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the documents into lists of nodes and serialize\n",
    "process_documents(documents, batch_size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = unpickle_nodes(\"/home/edouas/DATA-515/TLDhubeR/pickled_nodes/\")\n",
    "dump_object(nodes, \"nodes_full.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the nodes into a single list\n",
    "nodes = load_object(\"/home/edouas/DATA-515/TLDhubeR/pickled_nodes/nodes_full.pkl\")\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the correct metadata was exposed to the LLM and the embedding model\n",
    "print(nodes[0].get_content(metadata_mode=MetadataMode.LLM))\n",
    "print(nodes[0].get_content(metadata_mode=MetadataMode.EMBED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "# Construct vector store and customize storage context\n",
    "storage_context = StorageContext.from_defaults()\n",
    "\n",
    "# Create (or load) docstore and add nodes\n",
    "storage_context.docstore.add_documents(nodes)\n",
    "\n",
    "# Build index\n",
    "index = VectorStoreIndex(nodes, embed_model=Settings.embed_model, storage_context=storage_context)\n",
    "\n",
    "# Save index\n",
    "index.storage_context.persist(persist_dir=\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild storage context from directory\n",
    "storage_context_load = StorageContext.from_defaults(persist_dir=\"./data\")\n",
    "\n",
    "# Test reloading the index\n",
    "loaded_index = load_index_from_storage(storage_context_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing by configuring a retriever using this index\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=loaded_index,\n",
    "    similarity_top_k=20,\n",
    ")\n",
    "\n",
    "# Configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(response_mode=\"no_text\")\n",
    "\n",
    "# Assemble the query engine\n",
    "simple_hube_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.25)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing semantic searching (node retrieval) using a few sample queries\n",
    "response = simple_hube_engine.query(\"david goggins\")\n",
    "response.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = simple_hube_engine.query(\"The dangers of social media.\")\n",
    "response2.source_nodes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TLDHubeR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
